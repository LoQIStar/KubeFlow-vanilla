{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Pipeline Example\n",
    "The following notebook demonstrates the process of creating a Kubeflow Pipeline that trains and deploys a K-Means clustering algorithm on Amazon SageMaker infrastructure. The K-Means algorithm is trained on the MNIST dataset using SageMaker hyper-parameter tuning and batch transform jobs.\n",
    "\n",
    "Once the model has been trained sufficiently, it will be deployed as a model endpoint using on SageMaker. Inference can then be performed using this endpoint.\n",
    "\n",
    "The training, deployment and inference process described in this notebook has 4 steps:\n",
    "1. *S3 Bucket Creation & Data Upload*\n",
    "2. *Kubeflow Pipeline Creation*\n",
    "3. *Kubeflow Pipeline Deployment*\n",
    "4. *K-Means Model Endpoint Inference*\n",
    "\n",
    "Before proceeding, we will install the Kubeflow Pipelines SDK, `kfp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the Kubeflow Pipelines (kfp) package\n",
    "!python3 -m pip install kfp kfp-server-api --upgrade\n",
    "!pip show kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. S3 Bucket Creation & Data Upload\n",
    "In this section, we first create an S3 bucket that will store the data needed to train the K-Means algorithm, which will be MNIST. The MNIST dataset is downloaded from Amazon's SageMaker sample datasets S3 bucket. The dataset is unzipped, and split out into train, validation and test set folders.\n",
    "\n",
    "Before these folders are uploaded to the S3 bucket that we created, some data pre-processing must occur. More specifically the numpy arrays that describe the MNIST features will be converted to dense tensors are required by SageMaker's K-Means implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, string\n",
    "import boto3, gzip, pickle\n",
    "import numpy, urllib.request, json, io\n",
    "from urllib.parse import urlparse\n",
    "from sagemaker.amazon.common import write_numpy_to_dense_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "# Sets the AWS region\n",
    "AWS_REGION = 'eu-west-1'\n",
    "\n",
    "boto3_config = Config(region_name=AWS_REGION)\n",
    "\n",
    "# Initializes the S3 client\n",
    "s3_client = boto3.client('s3', config=boto3_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Bucket: ofw751-kubeflow-pipeline-data-bucket\n"
     ]
    }
   ],
   "source": [
    "# Creates a random hash to prefix to S3 bucket name\n",
    "HASH = ''.join([random.choice(string.ascii_lowercase) for n in range(3)] + [random.choice(string.digits) for n in range(3)])\n",
    "\n",
    "# Creates the S3 bucket name\n",
    "S3_BUCKET = '{}-kubeflow-pipeline-data-bucket'.format(HASH)\n",
    "\n",
    "# Creates the S3 bucket\n",
    "s3_client.create_bucket(Bucket=S3_BUCKET, \n",
    "                        CreateBucketConfiguration={\n",
    "                            'LocationConstraint': AWS_REGION\n",
    "                        })\n",
    "\n",
    "print('S3 Bucket:', S3_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the MNIST data from S3\n",
    "s3_client.download_file(f\"sagemaker-sample-data-{AWS_REGION}\", \n",
    "                        \"algorithms/kmeans/mnist/mnist.pkl.gz\", \n",
    "                        \"mnist.pkl.gz\")\n",
    "\n",
    "mnist_data = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(mnist_data, encoding='latin1')\n",
    "mnist_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data will be uploaded to: s3://ofw751-kubeflow-pipeline-data-bucket/mnist_kmeans_example/train_data\n",
      "\n",
      "Test data will be uploaded to: s3://ofw751-kubeflow-pipeline-data-bucket/mnist_kmeans_example/test_data\n"
     ]
    }
   ],
   "source": [
    "# Defines the train data key & the S3 path\n",
    "train_data_key = 'mnist_kmeans_example/train_data'\n",
    "train_data_location = 's3://{}/{}'.format(S3_BUCKET, train_data_key)\n",
    "print('Training data will be uploaded to: {}'.format(train_data_location))\n",
    "\n",
    "# Defines the test data key & the S3 path\n",
    "test_data_key = 'mnist_kmeans_example/test_data'\n",
    "test_data_location = 's3://{}/{}'.format(S3_BUCKET, test_data_key)\n",
    "print('\\nTest data will be uploaded to: {}'.format(test_data_location))\n",
    "\n",
    "# Converts the train data from numpy array to dense tensor (required by SageMaker K-Means algorithm)\n",
    "buf = io.BytesIO()\n",
    "write_numpy_to_dense_tensor(buf, train_set[0], train_set[1])\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(S3_BUCKET).Object(train_data_key).upload_fileobj(buf)\n",
    "\n",
    "# Converts the train data from numpy array to dense tensor (required by SageMaker K-Means algorithm)\n",
    "write_numpy_to_dense_tensor(buf, test_set[0], test_set[1])\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(S3_BUCKET).Object(test_data_key).upload_fileobj(buf)\n",
    "\n",
    "numpy.savetxt('valid-data.csv', \n",
    "              valid_set[0], \n",
    "              delimiter=',', \n",
    "              fmt='%g')\n",
    "\n",
    "input_key = \"{}/valid_data.csv\".format(\"mnist_kmeans_example/input\")\n",
    "\n",
    "# Uploads the validation data file to the S3 bucket\n",
    "s3_client.upload_file('valid-data.csv', S3_BUCKET, input_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Kubeflow Pipeline Creation\n",
    "Now that the MNIST data is ready and has been uploaded to the S3 bucket, the Kubeflow Pipeline can be created. The Kubeflow Pipeline will use the following resource components provided by SageMaker:\n",
    "- SageMaker Training Job component\n",
    "- SageMaker Hyper-parameter Tuning component\n",
    "- SageMaker Batch Transform Job component\n",
    "- SageMaker Model component\n",
    "- SageMaker Model Endpoint component\n",
    "\n",
    "The SageMaker K-Means algorithm is provided by AWS as a pre-built algorithm in the form of a Docker image which can be run on SageMaker as a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components, dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Training Job component\n",
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.1.1-beta.1/components/aws/sagemaker/train/component.yaml')\n",
    "\n",
    "# SageMaker Batch Transform Job component\n",
    "sagemaker_batch_transform_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.1.1-beta.1/components/aws/sagemaker/batch_transform/component.yaml')\n",
    "\n",
    "# SageMaker Hyper-parameter Tuning component\n",
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.1.1-beta.1/components/aws/sagemaker/hyperparameter_tuning/component.yaml')\n",
    "\n",
    "# SageMaker Model component\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.1.1-beta.1/components/aws/sagemaker/model/component.yaml')\n",
    "\n",
    "# SageMaker Model Endpoint component\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.1.1-beta.1/components/aws/sagemaker/deploy/component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list describes the regional addresses for the Amazon Elastic Container Registry (ECR) images that provide the SageMaker K-Means algorithm. In this example, we will use the image available in the `eu-west-1` (Irish) region.\n",
    "\n",
    "|Region| ECR Image|\n",
    "|------|----------|\n",
    "|us-west-1|632365934929.dkr.ecr.us-west-1.amazonaws.com|\n",
    "|us-west-2|174872318107.dkr.ecr.us-west-2.amazonaws.com|\n",
    "|us-east-1|382416733822.dkr.ecr.us-east-1.amazonaws.com|\n",
    "|us-east-2|404615174143.dkr.ecr.us-east-2.amazonaws.com|\n",
    "|us-gov-west-1|226302683700.dkr.ecr.us-gov-west-1.amazonaws.com|\n",
    "|ap-east-1|286214385809.dkr.ecr.ap-east-1.amazonaws.com|\n",
    "|ap-northeast-1|351501993468.dkr.ecr.ap-northeast-1.amazonaws.com|\n",
    "|ap-northeast-2|835164637446.dkr.ecr.ap-northeast-2.amazonaws.com|\n",
    "|ap-south-1|991648021394.dkr.ecr.ap-south-1.amazonaws.com|\n",
    "|ap-southeast-1|475088953585.dkr.ecr.ap-southeast-1.amazonaws.com|\n",
    "|ap-southeast-2|712309505854.dkr.ecr.ap-southeast-2.amazonaws.com|\n",
    "|ca-central-1|469771592824.dkr.ecr.ca-central-1.amazonaws.com|\n",
    "|eu-central-1|664544806723.dkr.ecr.eu-central-1.amazonaws.com|\n",
    "|eu-north-1|669576153137.dkr.ecr.eu-north-1.amazonaws.com|\n",
    "|eu-west-1|438346466558.dkr.ecr.eu-west-1.amazonaws.com|\n",
    "|eu-west-2|644912444149.dkr.ecr.eu-west-2.amazonaws.com|\n",
    "|eu-west-3|749696950732.dkr.ecr.eu-west-3.amazonaws.com|\n",
    "|me-south-1|249704162688.dkr.ecr.me-south-1.amazonaws.com|\n",
    "|sa-east-1|855470959533.dkr.ecr.sa-east-1.amazonaws.com|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies the appropriate ECR image\n",
    "KMEANS_ECR_IMAGE = '438346466558.dkr.ecr.eu-west-1.amazonaws.com'\n",
    "\n",
    "# Creates the S3 pipeline path\n",
    "S3_PIPELINE_PATH = 's3://{}/mnist_kmeans_example'.format(S3_BUCKET)\n",
    "\n",
    "# Specifies the SageMaker Execution Role ARN\n",
    "SAGEMAKER_ROLE_ARN = 'arn:aws:iam::920198949818:role/kubeflow-pipeline-sagemaker-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='kmeans-mnist-classification-pipeline',\n",
    "    description='K-Means MNIST Classification Pipeline'\n",
    ")\n",
    "\n",
    "def kmeans_mnist_classification_pipeline(region=AWS_REGION,\n",
    "    image=KMEANS_ECR_IMAGE+'/kmeans:1',\n",
    "    training_input_mode='File',\n",
    "    hpo_strategy='Bayesian',\n",
    "    hpo_metric_name='test:msd',\n",
    "    hpo_metric_type='Minimize',\n",
    "    hpo_early_stopping_type='Off',\n",
    "    hpo_static_parameters='{\"k\": \"10\", \"feature_dim\": \"784\"}',\n",
    "    hpo_integer_parameters='[{\"Name\": \"mini_batch_size\", \"MinValue\": \"500\", \"MaxValue\": \"600\"}, {\"Name\": \"extra_center_factor\", \"MinValue\": \"10\", \"MaxValue\": \"20\"}]',\n",
    "    hpo_continuous_parameters='[]',\n",
    "    hpo_categorical_parameters='[{\"Name\": \"init_method\", \"Values\": [\"random\", \"kmeans++\"]}]',\n",
    "    hpo_channels='[{\"ChannelName\": \"train\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/train_data\",  \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}, \\\n",
    "               {\"ChannelName\": \"test\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/test_data\", \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}]',\n",
    "    hpo_spot_instance='False',\n",
    "    hpo_max_wait_time='3600',\n",
    "    hpo_checkpoint_config='{}',\n",
    "    output_location=S3_PIPELINE_PATH + '/output',\n",
    "    output_encryption_key='',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count='1',\n",
    "    volume_size='50',\n",
    "    hpo_max_num_jobs='9',\n",
    "    hpo_max_parallel_jobs='2',\n",
    "    max_run_time='3600',\n",
    "    endpoint_url='',\n",
    "    network_isolation='True',\n",
    "    traffic_encryption='False',\n",
    "    train_channels='[{\"ChannelName\": \"train\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/train_data\",  \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}]',\n",
    "    train_spot_instance='False',\n",
    "    train_max_wait_time='3600',\n",
    "    train_checkpoint_config='{}',\n",
    "    batch_transform_instance_type='ml.m4.xlarge',\n",
    "    batch_transform_input=S3_PIPELINE_PATH + '/input',\n",
    "    batch_transform_data_type='S3Prefix',\n",
    "    batch_transform_content_type='text/csv',\n",
    "    batch_transform_compression_type='None',\n",
    "    batch_transform_ouput=S3_PIPELINE_PATH + '/output',\n",
    "    batch_transform_max_concurrent='4',\n",
    "    batch_transform_max_payload='6',\n",
    "    batch_strategy='MultiRecord',\n",
    "    batch_transform_split_type='Line',\n",
    "    role_arn=SAGEMAKER_ROLE_ARN\n",
    "    ):\n",
    "\n",
    "    hpo = sagemaker_hpo_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        image=image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        strategy=hpo_strategy,\n",
    "        metric_name=hpo_metric_name,\n",
    "        metric_type=hpo_metric_type,\n",
    "        early_stopping_type=hpo_early_stopping_type,\n",
    "        static_parameters=hpo_static_parameters,\n",
    "        integer_parameters=hpo_integer_parameters,\n",
    "        continuous_parameters=hpo_continuous_parameters,\n",
    "        categorical_parameters=hpo_categorical_parameters,\n",
    "        channels=hpo_channels,\n",
    "        output_location=output_location,\n",
    "        output_encryption_key=output_encryption_key,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_num_jobs=hpo_max_num_jobs,\n",
    "        max_parallel_jobs=hpo_max_parallel_jobs,\n",
    "        max_run_time=max_run_time,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=hpo_spot_instance,\n",
    "        max_wait_time=hpo_max_wait_time,\n",
    "        checkpoint_config=hpo_checkpoint_config,\n",
    "        role=role_arn,\n",
    "    )\n",
    "\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        image=image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        hyperparameters=hpo.outputs['best_hyperparameters'],\n",
    "        channels=train_channels,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        model_artifact_path=output_location,\n",
    "        output_encryption_key=output_encryption_key,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=train_spot_instance,\n",
    "        max_wait_time=train_max_wait_time,\n",
    "        checkpoint_config=train_checkpoint_config,\n",
    "        role=role_arn,\n",
    "    )\n",
    "\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name=training.outputs['job_name'],\n",
    "        image=training.outputs['training_image'],\n",
    "        model_artifact_url=training.outputs['model_artifact_url'],\n",
    "        network_isolation=network_isolation,\n",
    "        role=role_arn\n",
    "    )\n",
    "\n",
    "    prediction = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name_1=create_model.output,\n",
    "    )\n",
    "\n",
    "    batch_transform = sagemaker_batch_transform_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name=create_model.output,\n",
    "        instance_type=batch_transform_instance_type,\n",
    "        instance_count=instance_count,\n",
    "        max_concurrent=batch_transform_max_concurrent,\n",
    "        max_payload=batch_transform_max_payload,\n",
    "        batch_strategy=batch_strategy,\n",
    "        input_location=batch_transform_input,\n",
    "        data_type=batch_transform_data_type,\n",
    "        content_type=batch_transform_content_type,\n",
    "        split_type=batch_transform_split_type,\n",
    "        compression_type=batch_transform_compression_type,\n",
    "        output_location=batch_transform_ouput\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Kubeflow Pipeline Deployment\n",
    "In v1.1.0 of Kubeflow, in-cluster communication from the notebook server to the Kubeflow Pipeline is not currently supported. A workaround is to pass a session cookie to the `kfp` SDK so that it can communicate with the cluster. More documentation is provided [here](https://www.kubeflow.org/docs/aws/pipeline/).\n",
    "\n",
    "In the meantime, the simplest thing to do is compile the Kubeflow Pipeline in the notebook which will generate a zip file called `kmeans-mnist-classification-pipeline.zip`. This zip file can be used to manually deploy the Pipeline in Kubeflow Console by following these steps:\n",
    "1. Run the following cell to compile the Kubeflow Pipeline and you should see a `kmeans-mnist-classification-pipeline.zip` file appear in Jupyter.\n",
    "2. Download the `kmeans-mnist-classification-pipeline.zip` file to your local machine.\n",
    "3. In the Kubeflow Console, click the *Pipelines* button.\n",
    "4. In the Pipelines UI, click the *Upload Pipeline button* on the top right.\n",
    "5. For *Pipeline Name*, type `kmeans-mnist-classification-pipeline`.\n",
    "6. Select the *Upload Pipeline* button and upload the `kmeans-mnist-classification-pipeline.zip` file from your Downloads folder and click the *Create* button.\n",
    "7. Now that the Pipeline is ready, an *Experiment* can be created by clicking the *Create experiment* button.\n",
    "8. For the *Experiment name*, type `kmeans_mnist_classification_experiment` and click the *Next* button and you will be re-directed to the *Start a run* view where you can initialise an Experiment Run.\n",
    "9. Experiment names must be unique so if that name already exists, modify it to make it unique.\n",
    "10. In the *Start a run* view, choose the `kmeans-mnist-classification-pipeline` Pipeline that you previously created.\n",
    "11. For the *Run name*, type `kmeans-mnist-classification-pipeline-run`.\n",
    "12. Leave the remaining defaults unchanged and click the *Start* button.\n",
    "13. In the *Runs* view, click the Run that you have just created and you should see the directed acyclic graph (DAG) that visually represents the Kubeflow Pipeline that you have created.\n",
    "14. The run will take around 15-20 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiles the Kubeflow Pipeline\n",
    "kfp.compiler.Compiler().compile(kmeans_mnist_classification_pipeline, \n",
    "                                'kmeans-mnist-classification-pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can skip the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alb_cookie_content='<cookie-content-here>'\n",
    "\n",
    "# authservice_session='authservice_session='+alb_cookie_content\n",
    "\n",
    "# alb_dns = 'http://8cd1d44c-istiosystem-istio-2af2-1274113137.eu-west-1.elb.amazonaws.com'\n",
    "\n",
    "# client = kfp.Client(host=alb_dns+'/pipeline', \n",
    "#                     cookies=authservice_session)\n",
    "\n",
    "# # Creates a Kubeflow Experiment\n",
    "# kmeans_mnist_classification_experiment = client.create_experiment(name='kmeans_mnist_classification_experiment', \n",
    "#                                                                   namespace='kflow-user')\n",
    "\n",
    "# # Runs the Kubeflow Pipeline\n",
    "# kmeans_mnist_classification_pipeline_run = client.run_pipeline(kmeans_mnist_classification_experiment.id,\n",
    "#                                                                'kmeans-mnist-classification-pipeline-run', \n",
    "#                                                                'kmeans-mnist-classification-pipeline-run.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. K-Means Model Endpoint Inference\n",
    "Once the Experiment Run has successfully completed, an Amazon SageMaker Model Endpoint should have been successfully deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker', \n",
    "                                config=boto3_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Model Endpoint: Endpoint20201217142527-L123\n"
     ]
    }
   ],
   "source": [
    "# Parses the SageMaker Model Endpoint name\n",
    "kmeans_model_endpoint = sagemaker_client.list_endpoints()['Endpoints'][0]['EndpointName']\n",
    "print('K-Means Model Endpoint:', kmeans_model_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2csv(arr):\n",
    "    '''Creates a CSV file from the Numpy array'''\n",
    "    csv = io.BytesIO()\n",
    "    numpy.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'distance_to_cluster': 7.2547078132629395, 'closest_cluster': 5.0}]}\n"
     ]
    }
   ],
   "source": [
    "# Initialises the SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime',\n",
    "                                 config=boto3_config)\n",
    "\n",
    "# Invokes the np2csv function to create a payload to perform inference on\n",
    "payload = np2csv(train_set[0][30:31])\n",
    "\n",
    "# Invokes the SageMaker Model Endpoint and captures the response\n",
    "kmeans_model_endpoint_response = sagemaker_runtime.invoke_endpoint(EndpointName=kmeans_model_endpoint,\n",
    "                                                      ContentType='text/csv',\n",
    "                                                      Body=payload)\n",
    "\n",
    "prediction = json.loads(kmeans_model_endpoint_response['Body'].read().decode())\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Cleanup\n",
    "The following resources should be deleted once you have finished with the notebook:\n",
    "- SageMaker Model Endpoint\n",
    "- SageMaker Model Endpoint Configuration\n",
    "- SageMaker Model\n",
    "- S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Model: TrainingJob-20201217142147-IDXG\n"
     ]
    }
   ],
   "source": [
    "# Parses the SageMaker Model name\n",
    "kmeans_model = sagemaker_client.list_models()['Models'][0]['ModelName']\n",
    "print('K-Means Model:', kmeans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Model Endpoint Config: EndpointConfig20201217142527-L123\n"
     ]
    }
   ],
   "source": [
    "# Parses the SageMaker Model Endpoint Config name\n",
    "kmeans_model_endpoint_config = sagemaker_client.describe_endpoint(EndpointName=kmeans_model_endpoint)['EndpointConfigName']\n",
    "print('K-Means Model Endpoint Config:', kmeans_model_endpoint_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '233e427a-4293-4c32-aa3a-56d10842090a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '233e427a-4293-4c32-aa3a-56d10842090a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 17 Dec 2020 14:56:41 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deletes the SageMaker Model Endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=kmeans_model_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '11ab6a20-4fd7-4a3b-bd07-83105bdaaac3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '11ab6a20-4fd7-4a3b-bd07-83105bdaaac3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 17 Dec 2020 14:56:44 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deletes the SageMaker Model Endpoint Config\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=kmeans_model_endpoint_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e1e8a66b-2605-4776-8838-e10c4799dd57',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e1e8a66b-2605-4776-8838-e10c4799dd57',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 17 Dec 2020 14:56:45 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deletes the SageMaker Model\n",
    "sagemaker_client.delete_model(ModelName=kmeans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '46412DA4034226AB',\n",
       "  'HostId': '3ywwaHCDSSQ/UWr6AZI1/iTb5DasrbDUmSf6u92UiOeRb5woIJzv+F1+tlRx+kucAKwLKR5A6JM=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '3ywwaHCDSSQ/UWr6AZI1/iTb5DasrbDUmSf6u92UiOeRb5woIJzv+F1+tlRx+kucAKwLKR5A6JM=',\n",
       "   'x-amz-request-id': '46412DA4034226AB',\n",
       "   'date': 'Thu, 17 Dec 2020 14:56:50 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data_bucket = boto3.resource('s3').Bucket(S3_BUCKET)\n",
    "\n",
    "# Deletes all of the objects in S3 bucket + deletes the bucket\n",
    "mnist_data_bucket.objects.all().delete()\n",
    "mnist_data_bucket.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}